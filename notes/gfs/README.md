# GFS

## 概要

我们设计并实现了Google文件系统，一个可扩展的分布式文件系统，用于大型分布式数据密集型应用程序。它在运行于廉价的商品硬件上时提供了容错能力，并为大量客户机提供了高聚合性能。

虽然与以前的分布式文件系统有许多相同的目标，但我们的设计是由对当前和预期的应用程序工作负载和技术环境的观察所驱动的，这些都反映出与一些早期的文件系统假设的明显背离。这让我们重新审视传统的选择，探索完全不同的设计点。

该文件系统已经成功地满足了我们的存储需求。我们的研究和数据处理平台需要大量的数据存储和处理。迄今为止最大的集群在一千多台机器上的数千个磁盘上提供了数百TB的存储，并且可以由数百个客户机同时访问。

在这篇文章中，我们介绍了为支持分布式应用程序而设计的文件系统接口扩展，讨论了我们设计的许多方面，并报告了从微观基准测试和实际使用中进行的测量。

## 关键字

容错、可扩展性、数据存储、集群存储

## 1.简介

我们设计并实现了Google文件系统（GFS），以满足Google快速增长的数据处理需求。GFS与以前的分布式文件系统有许多相同的目标，比如性能、可扩展性、可靠性和可用性。然而，它的设计是由对我们当前和预期的应用程序工作负载和技术环境的关键观察结果驱动的，这些都反映出与一些早期的文件系统设计假设的明显偏离。我们重新审视了传统的选择，并探索了设计空间中完全不同的点。

首先，部件故障是正常现象，而不是例外。文件系统由数百甚至数千台存储机器组成，这些存储机器由廉价的商品部件构建而成，可供相当数量的客户机访问。组件的数量和质量实际上保证了某些组件在任何给定时间都无法正常工作，有些组件无法从当前的故障中恢复。我们已经看到了由应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障引起的问题。因此，必须将持续监控、错误检测、容错和自动恢复整合到系统中。

第二，按传统标准，文件非常庞大。多GB文件很常见。每个文件通常包含许多应用程序对象，如web文档。当我们经常使用由数十亿个对象组成的TB级的快速增长的数据集时，即使文件系统可以支持它，管理数十亿个大约KB大小的文件也很难管理。因此，必须重新考虑设计假设和参数，如I/O操作和块大小

第三，大多数文件都是通过附加(appending)新数据而不是覆盖现有数据来改变的。文件中的随机写入实际上是不存在的。一旦写入，文件就只能被读取，而且通常只能按顺序读取。各种数据共享这些特征。有些可能构成数据分析程序扫描的大型存储库。有些可能是运行应用程序连续生成的数据流。有些可能是存档数据。有些可能是在一台机器上产生并在另一台机器上进行处理的中间结果，无论是同时进行的还是以后的。考虑到这种对大文件的访问模式，附加成为性能优化和原子性保证的焦点，而在客户端缓存数据块则失去了吸引力。

第四，联合设计应用程序和文件系统API，通过增加灵活性，使整个系统受益。例如，我们放宽了GFS的一致性模型，以大大简化文件系统，而不会给应用程序带来沉重的负担。我们还引入了一个原子附加操作，这样多个客户机可以并发地追加到一个文件中，而不需要它们之间的额外同步。这些将在本文后面进行更详细的讨论。

目前部署了多个GFS集群用于不同的目的。最大的一个有1000多个存储节点，超过300 TB的磁盘存储，并且有数百个客户机连续地在不同的机器上进行大量访问。

## 2.设计概览

### 2.1 假设

在为我们的需求设计一个文件系统时，我们一直遵循既能带来挑战又能带来机遇的假设。我们之前提到了一些关键的观察结果，现在更详细地阐述了我们的假设。

+ 该系统由许多廉价的商品组件构成，这些组件通常会出现故障。它必须不断地自我监控，并在常规基础上检测、容忍和迅速从组件故障中恢复。
+ 系统存储少量大文件。我们希望有几百万个文件，每个文件的大小通常是100 MB或更大。多GB文件是常见的情况，应该进行有效的管理。必须支持小文件，但我们不需要为它们进行优化。
+ 工作负载主要由两种类型的读取组成：大型流式读取和小型随机读取。在大型流式读取中，单个操作通常读取数百KB，更常见的是1 MB或更多。来自同一客户机的连续操作通常会读取文件的一个连续区域。一个小的随机读取通常以任意偏移量读取几个KBs。注重性能的应用程序通常会对小的读取进行批处理和排序，以稳定地在文件中前进，而不是来回地进行。
+ 工作负载还有许多大的、顺序的写入操作，它们将数据附加到文件中。典型的操作尺寸与读取操作相似。一旦写入，文件很少再被修改。支持文件中任意位置的小的写操作，但不一定要高效。
+ 系统必须为同时附加到同一文件的多个客户端高效地实现定义良好的语义。我们的文件通常用作生产者-消费者队列或用于多种方式的合并。数百个生产者，每台机器运行一个，将并发地附加到一个文件中。原子性和最小的同步开销是必不可少的。稍后可能会读取该文件，或者使用者可能正在同时读取该文件。
+ 高持续带宽比低延迟更重要。我们的大多数目标应用程序都重视以高速率批量处理数据，而很少有应用程序对单个读或写有严格的响应时间要求。

### 2.2 接口

GFS提供了一个熟悉的文件系统接口，尽管它没有实现像POSIX这样的标准API。文件在目录中按层次组织，并由路径名标识。我们支持`create`、`delete`、`open`、`close`、`read`和`write`文件的常规操作。

此外，GFS有`snapshot`和`record append`操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户机同时向同一个文件追加数据，同时保证每个客户机的`append`的原子性。它对于实现多路合并结果和生产者-消费者队列非常有用，许多客户机可以同时追加到这些队列中，而无需额外锁定。我们发现这些类型的文件在构建大型分布式应用程序时非常有价值。快照和记录追加分别在第3.4节和第3.3节中进一步讨论

### 2.3 架构

![img](https://github.com/Qasak/distributed-system-notes-and-labs/blob/master/notes/gfs/GFS%E6%9E%B6%E6%9E%84.png)

GFS集群由一个`master`和多个`chunkserver`组成，由多个`clients`访问，如图1所示。其中每一个都是运行用户级服务器进程的普通Linux机器。在同一台机器上同时运行`chunkserver`和客户机是很容易的，只要机器资源允许，并且运行可能不可靠的应用程序代码所导致的低可靠性是可以接受的。

文件被分成固定大小的块(chunks)。每个块由主块在创建块时分配的不可变且全局唯一的64位块句柄(chunk handle)标识。`Chunkservers`将本地磁盘上的块存储为Linux文件，并读取或写入由块句柄和字节范围指定的块数据。为了提高可靠性，每个块都复制到多个`chunkserver`上。默认情况下，我们存储三个副本，但是用户可以为文件命名空间的不同区域指定不同的复制级别。

主服务器(master)维护所有文件系统元数据。这包括命名空间、访问控制信息、从文件到块的映射以及块的当前位置。它还控制系统范围内的活动，如块租赁管理(chunk lease)、孤立块的垃圾收集以及块服务器之间的块迁移。主机周期性地通过心跳消息(`HeartBeat`)与每个`chunkserver`通信，以向其发出指令并收集其状态。

链接到每个应用程序的GFS客户机代码实现文件系统API，并与主服务器和`chunkserver`通信以代表应用程序读取或写入数据。客户机与主服务器交互进行元数据操作，但是所有承载数据的通信都直接发送到`chunkserver`。我们不提供POSIX API，因此不需要挂接到linux vnode层。

客户机和`chunkserver`都不缓存文件数据。客户机缓存没有什么好处，因为大多数应用程序都是通过大型文件流式传输的，或者工作集太大而无法缓存。不使用它们可以消除缓存一致性问题，从而简化客户端和整个系统。（但是客户端缓存元数据）`Chunkservers`不需要缓存文件数据，因为块存储为本地文件，因此Linux的缓冲区缓存已经将频繁访问的数据保存在内存中。

### 2.4 单Master

拥有一个主节点可以大大简化我们的设计，并使主节点能够利用全局知识做出复杂的块放置和复制决策。然而，我们必须尽量减少它在读写方面的参与，以免它成为瓶颈。客户机从不通过主机读写文件数据。相反，客户端会询问主服务器它应该联系哪个`chunkserver`。它在有限的时间内缓存这些信息，并直接与`chunkserver`交互以进行后续操作。

让我们参考图1来解释简单read的交互。首先，使用固定的块大小，客户机将应用程序指定的文件名和字节偏移量转换为文件内的块索引。然后，它向主机发送一个包含文件名和块索引的请求。主服务器用相应的块句柄和副本的位置进行答复。客户端使用文件名和块索引作为键来缓存这些信息

然后，客户机向其中一个副本（很可能是最近的副本）发送请求。请求指定块句柄和块内的字节范围。在缓存信息过期或文件重新打开之前，对同一块的进一步读取不需要更多的客户端-主机交互。实际上，客户机通常在同一个请求中请求多个块，主块也可以在请求后立即包含块的信息。这些额外的信息避免了未来的几次客户机-主机交互，几乎没有额外的成本

### 2.5 块大小

块大小是关键的设计参数之一。我们选择了64MB，这比典型的文件系统块大小大得多。每个块复制副本都作为纯Linux文件存储在`chunkserver`上，并且只在需要时进行扩展。延迟空间(lazy space allocation)分配避免了由于内部碎片而浪费空间，这可能是反对如此大的块大小的最大障碍。

> 文件系统将磁盘空间划分为每1024个字节一组，称为块(也有用512字节为一块的，如：SCOXENIX）

大的块尺寸有几个重要的优点。首先，它减少了客户机与主机交互的需要，因为在同一块上读写只需要向主机发出一个初始请求，请求提供块位置信息。这种减少对于我们的工作负载特别重要，因为应用程序通常按顺序读写大文件。即使对于小的随机读取，客户机也可以轻松地缓存多TB工作集的所有块位置信息。其次，由于在一个较大的块上，客户机更有可能在给定的块上执行许多操作，因此它可以通过在较长时间内保持与`chunkserver`的持久TCP连接来减少网络开销。第三，它减少了存储在主机上的元数据的大小。这允许我们将元数据保存在内存中，这反过来带来了我们将在第2.6.1节中讨论的其他优点。

另一方面，较大的块大小，即使使用延迟空间分配，也有其缺点。一个小文件由少量的块组成，也许只有一个。如果许多客户机正在访问同一个文件，那么存储这些块的`chunkserver`可能会成为热点。在实践中，热点并不是一个主要问题，因为我们的应用程序大多按顺序读取大型多块文件

然而，当GFS第一次被批处理队列系统使用时，热点确实成为了问题：一个可执行文件被作为单个块文件写入GFS，然后同时在数百台机器上启动。存储此可执行文件的少数`chunkserver`由于数百个同时请求而过载。我们通过使用更高的复制因子存储这些可执行文件，并使批处理队列系统错开应用程序的启动时间，解决了这个问题。一个潜在的长期解决方案是允许客户端在这种情况下从其他客户端读取数据。

### 2.6 元数据 Metadata

master存储三种主要类型的元数据：文件和块名称空间、从文件到块的映射以及每个块副本的位置。所有元数据都保存在主机的内存中。前两种类型（名称空间和文件到块映射）也可以通过将改变记录到存储在master本地磁盘上并复制到远程计算机上的操作日志(`operation log`)来保持持久性。使用日志可以让我们简单、可靠地更新master状态，并且不会在主崩溃时出现不一致的风险。主机不会永久存储块位置信息。相反，它会在master启动时以及每当`chunkserver`加入集群时询问每个`chunkserver`关于它的块的信息。

#### 2.6.1 内存中的数据结构

由于元数据存储在内存中，master操作速度很快。此外，master很容易高效地定期扫描后台的整个状态。这种定期扫描用于实现块垃圾收集、在`chunkserver`出现故障时重新复制以及块迁移，以平衡`chunkserver`的负载和磁盘空间使用。第4.3和4.4节将进一步讨论这些活动。

这种只使用内存的方法的一个潜在问题是，块的数量以及整个系统的容量都受到主机内存量的限制。这在实践中并不是一个严重的限制。主数据块的元数据小于64 MB。大多数块是满的，因为大多数文件包含许多块，只有最后一块可能被部分填充。

类似地，文件名称空间数据对于每个文件通常需要少于64字节，因为它使用前缀压缩紧凑地存储文件名。如果需要支持更大的文件系统，那么向master添加额外内存的成本是一个很小的代价，因为我们通过将元数据存储在内存中而获得的简单性、可靠性、性能和灵活性

#### 2.6.2 块的位置

master不保存一个持久的记录，记录哪些`chunkserver`有一个给定块的副本。它只是在启动时轮询`chunkserver`以获取这些信息。主机可以在此后保持自己的最新状态，因为它控制所有块的放置，并通过常规的心跳消息监视`chunkserver`状态。

我们最初试图将块位置信息持久地保存在master上，但是我们发现在启动时从`chunkservers`请求数据要简单得多，之后还要定期请求。这消除了在`chunkservers`加入和离开集群、更改名称、失败、重新启动等过程中保持主服务器和`chunkserver`同步的问题。在拥有数百台服务器的集群中，这些事件经常发生。

理解这种设计决策的另一种方法是认识到`chunkserver`对它自己的磁盘上有哪些块拥有最终决定权。试图在主服务器上保持一致的信息视图是没有意义的，因为`chunkserver`上的错误可能会导致块自动消失（例如，磁盘可能会坏掉并被禁用），或者操作员可能会重命名`chunkserver`

#### 2.6.3 操作日志

操作日志包含关键元数据更改的历史记录。它是GFS的核心。它不仅是元数据的唯一持久记录，而且还是定义并发操作顺序的逻辑时间线。文件和块，以及它们的版本（见第4.5节），都是由它们创建时的逻辑时间唯一和永恒地标识的。

由于操作日志是关键的，我们必须可靠地存储它，并且在元数据更改并持久化之后，才能使（对操作日志的）更改对客户端可见。否则，即使块本身仍然存在，我们实际上也会丢失整个文件系统或最近的客户端操作。因此，我们在多台远程机器上复制它，并且只在本地和远程地将相应的日志记录刷新到磁盘之后才响应客户机操作。master在刷新之前将多个日志记录批处理在一起，从而减少了刷新和复制对整个系统吞吐量的影响

master通过重放操作日志恢复其文件系统状态。为了减少启动时间，我们必须保持日志小。当日志增长超过某个大小时，主检查点就为其状态设置一个检查点，以便通过从本地磁盘加载最新的检查点并在此之后仅回放有限数量的日志记录来恢复。检查点是一种紧凑的B树型形式，可以直接映射到内存中，用于名称空间查找，而无需额外的解析。这进一步加快了恢复速度并提高了可用性。

因为构建一个检查点可能需要一段时间，所以主节点的内部状态的结构是这样的：在不延迟新来的改变情况下创建新的checkpoint。主服务器切换到新的日志文件，并在单独的线程中创建新的检查点。新的检查点包括切换之前的所有改变。对于一个包含几百万个文件的集群，它可以在一分钟左右创建。完成后，同时写入本地和远程磁盘。

恢复只需要最新的完整检查点和后续日志文件。旧的检查点和日志文件可以自由删除，尽管我们保留了一些以防止灾难发生。检查点期间的故障不会影响正确性，因为恢复代码会检测并跳过不完整的检查点。

### 2.7 一致性模型

GFS有一个宽松的一致性模型，它很好地支持我们的高度分布式应用程序，但实现起来仍然相对简单和高效。现在我们将讨论GFS的保证及其对应用程序的意义。我们还强调了GFS是如何维持这些保证的，但具体细节将留给本文的其他部分。

#### 2.7.1 GFS的保障

文件名称空间的变化（例如，文件创建）是原子的。它们完全由master处理：名称空间锁保证了原子性和正确性（第4.1节）；主机的操作日志定义了这些操作的全局总顺序（第2.6.3节）。

数据改变后文件区域的状态取决于改变的类型、它是成功还是失败，以及是否存在并发改变。表1总结了结果。如果所有客户机始终看到相同的数据，则文件区域是一致的，而不管它们从哪个副本中读取数据。如果一个区域是一致的，则在文件数据变异之后定义一个区域，并且客户机将看到该变异所写的全部内容。当一个变异在没有并发作者干扰的情况下成功时，受影响的区域就被定义了（并且隐含一致）：所有的客户都会看到突变写了什么。同时成功的突变使得这个区域没有定义，但是一致性：所有的客户都看到相同的数据，但是它可能不能反映任何一个突变所写的内容。一般来说，它由多个突变的混合片段组成。失败的变异使区域不一致（因此也未定义）：不同的客户端可能在不同的时间看到不同的数据。下面我们将介绍应用程序如何区分已定义区域和未定义区域。应用程序不需要进一步区分不同类型的未定义区域。数据突变可能是写入或记录附加。写入操作导致数据以应用程序指定的文件偏移量写入。记录追加导致数据（即“记录”）至少被原子性地追加一次，即使是在同时发生突变的情况下，但要以GFS选择的偏移量（第3.3节）。（相比之下，“常规”追加只是在客户机认为是文件当前结尾的偏移量处写入）偏移量返回给客户机并标记包含记录的已定义区域的开始。此外，GFS可以在中间插入填充或重复记录。它们所占据的区域被认为是不一致的，通常与用户数据量相比相形见绌。在一系列成功的变异之后，变异的文件区域被保证被定义并且包含最后一次变异所写的数据。GFS通过（a）在其所有副本上以相同的顺序对一个块应用突变来实现这一点（第3.1节），和（b）使用块版本号来检测任何由于其chunkserver关闭而丢失了突变而变得过时的副本（第4.5节）。过时的副本永远不会涉及到变异，也不会提供给客户机，要求主机提供块位置。它们被尽早地收集起来。由于客户端缓存块位置，因此在刷新信息之前，它们可能会从过时的副本中读取数据。此窗口受缓存项的超时和下次打开文件的时间限制，这将从缓存中清除该文件的所有块信息。此外，由于我们的大多数文件都是只追加的，过时的副本通常会返回一个过早结束的数据块，而不是过时的数据。当读卡器重试并与主服务器联系时，它将立即获取当前块的位置。在成功的变异之后很长一段时间，组件故障当然仍然会损坏或破坏数据。GFS通过主服务器和所有chunkserver之间的定期握手来识别失败的chunkserver，并通过校验和来检测数据损坏（第5.2节）。一旦出现问题，将尽快从有效副本中恢复数据（第4.3节）。只有在GFS能够做出反应之前（通常在几分钟内），一个块的所有副本都丢失了，才会不可逆转地丢失。即使在这种情况下，它也变得不可用，不会损坏：应用程序接收的是明确的错误，而不是损坏的数据。